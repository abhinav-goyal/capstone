{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "__doc__ = \"\"\"\n",
    "    This file was first used for working on the Single/Cropped Images and contains \n",
    "    the data preparation steps for that dataset. However, the really interesting neural networks \n",
    "    bits are present in SingleImages_Explore. The implementation here is more of a sanity check.\n",
    "    \n",
    "    Please look at SingleImages_Explore for a more comprehensive study of the dataset.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### import the required libraries\n",
    "\n",
    "import numpy as np\n",
    "import scipy.ndimage as ndimage\n",
    "import scipy.io as sio\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import math\n",
    "import os\n",
    "import pylab as P\n",
    "import skimage\n",
    "from sklearn.cross_validation import train_test_split as ttsplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## define the directory where the data is\n",
    "\n",
    "# go to the parent of the current dir\n",
    "data_parent = os.path.split(os.getcwd())[0]\n",
    "data_path = os.path.join(data_parent, 'data')\n",
    "if not os.path.exists(data_path):\n",
    "   print \"please define the data_path variable to where data sits\"\n",
    "   raise e\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_mat = os.path.join(data_path, 'train_32x32.mat')\n",
    "test_mat = os.path.join(data_path, 'test_32x32.mat')\n",
    "extra_mat = os.path.join(data_path, 'extra_32x32.mat')\n",
    "train_data  = sio.loadmat(train_mat)\n",
    "test_data = sio.loadmat(test_mat)\n",
    "extra_data  = sio.loadmat(extra_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The images are stored in X and the labels are stored in y\n",
    "IMAGE_KEY = 'X'\n",
    "LABEL_KEY = 'y'\n",
    "print train_data[IMAGE_KEY].shape\n",
    "print train_data[LABEL_KEY].shape\n",
    "\n",
    "print test_data[IMAGE_KEY].shape\n",
    "print test_data[LABEL_KEY].shape\n",
    "\n",
    "print extra_data[IMAGE_KEY].shape\n",
    "print extra_data[LABEL_KEY].shape\n",
    "\n",
    "# The sizes match - good first step\n",
    "TRAIN_DATASET_SIZE = train_data[LABEL_KEY].shape[0]\n",
    "TEST_DATASET_SIZE  = test_data[LABEL_KEY].shape[0]\n",
    "EXTRA_DATASET_SIZE = extra_data[LABEL_KEY].shape[0]## Get an idea of the labels we are dealing with\n",
    "train_label_set = set(train_data[LABEL_KEY].reshape(TRAIN_DATASET_SIZE))\n",
    "test_label_set = set(test_data[LABEL_KEY].reshape(TEST_DATASET_SIZE))\n",
    "extra_label_set = set(extra_data[LABEL_KEY].reshape(EXTRA_DATASET_SIZE))\n",
    "assert(train_label_set == test_label_set)\n",
    "assert(train_label_set == extra_label_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Hurray - no mismatching labels! Phew! Always awkward when that happens\n",
    "## On second thoughts, this is guaranteed to be a well curated data set (thanks, Google!)\n",
    "## Don't need to run basic sanity checks\n",
    "\n",
    "print train_label_set## The labels are from 1 to 10, as already explained on the SVHN page.\n",
    "## we will replace 10 by 0\n",
    "train_labels_replace = train_data[LABEL_KEY].reshape(TRAIN_DATASET_SIZE)\n",
    "train_labels_replace[train_labels_replace == 10]  = 0\n",
    "train_labels_replace = train_labels_replace.astype(np.int32)\n",
    "\n",
    "extra_labels = extra_data[LABEL_KEY].reshape(EXTRA_DATASET_SIZE)\n",
    "extra_labels[extra_labels == 10] = 0\n",
    "extra_labels_replace = extra_labels.astype(np.int32)\n",
    "\n",
    "test_labels = test_data[LABEL_KEY].reshape(TEST_DATASET_SIZE).astype(np.int32)\n",
    "test_labels[test_labels==10] = 0\n",
    "\n",
    "print set(train_labels_replace)\n",
    "assert(set(train_labels_replace) == set(test_labels))\n",
    "assert(set(extra_labels) == set(train_labels_replace))\n",
    "\n",
    "num_labels = len(train_label_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## cleaned up the labels for use -> now, let's work work with the images themselves\n",
    "## The vector is provided 32, 32, 3, 73257 -> let's see how this looks\n",
    "\n",
    "display_size  = 10\n",
    "rand_indices = np.random.randint(0, TRAIN_DATASET_SIZE, size=display_size)\n",
    "fig = P.figure(figsize=(20, 20))\n",
    "for idx in range(len(rand_indices)):\n",
    "    a=fig.add_subplot(1,display_size,idx+1)\n",
    "    P.imshow(train_data[IMAGE_KEY][:,:,:,rand_indices[idx]])\n",
    "    a.set_title(str(train_labels_replace[rand_indices[idx]]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Let's get on with it. we need to move the indices around in a way in which we understand them \n",
    "## we want the first index to select the image, the second to select a row, the third to select a column\n",
    "## and the fourth to select the colour (r,g,b)\n",
    "\n",
    "## since the images are all aligned correctly, simply transpose the last index to the first\n",
    "train_dataset_unnorm = train_data[IMAGE_KEY].transpose((3,0,1,2)).astype(np.float32)\n",
    "test_images_unnorm  = test_data[IMAGE_KEY].transpose((3,0,1,2)).astype(np.float32)\n",
    "extra_images_unnorm = extra_data[IMAGE_KEY].transpose((3,0,1,2)).astype(np.float32)#train_dataset = np.concatenate((train_dataset, extra_images), axis = 0)\n",
    "#train_labels_replace = np.concatenate((train_labels_replace, extra_labels), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print train_dataset_unnorm[0, 0, 0, :]\n",
    "print test_images_unnorm[0, 0, 0, :]\n",
    "print extra_images_unnorm[0, 0, 0, :]\n",
    "\n",
    "def convert_to_grayscale(images):\n",
    "    \"\"\"\n",
    "        https://groups.google.com/forum/#!topic/sci.image.processing/Jf-aTjPEgjc\n",
    "    \"\"\"\n",
    "    return np.dot(images, [[0.299],[0.587],[0.114]])\n",
    "\n",
    "def normalize(ds):\n",
    "    ds = convert_to_grayscale(ds)\n",
    "    return (ds  - np.mean(ds))/np.std(ds) \n",
    "    #mid_p)/pixel_depth\n",
    "    #return skimage.img_as_float(ds)\n",
    "\n",
    "train_dataset = normalize(train_dataset_unnorm).astype(np.float32)\n",
    "test_images = normalize(test_images_unnorm).astype(np.float32)\n",
    "extra_images = normalize(extra_images_unnorm).astype(np.float32)\n",
    "\n",
    "print np.mean(train_dataset), np.std(train_dataset)\n",
    "print np.mean(test_images), np.std(test_images)\n",
    "print np.mean(extra_images), np.std(extra_images)\n",
    "\n",
    "print train_dataset[0, 0, 0, :]\n",
    "print test_images[0, 0, 0, :]\n",
    "print extra_images[0, 0, 0, :]\n",
    "print train_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(len(train_label_set)):\n",
    "    print(i, len(train_labels_replace[train_labels_replace == i]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## display images again to make sure that the index is now in the correct position\n",
    "rand_indices = np.random.randint(0, TRAIN_DATASET_SIZE, size=display_size)\n",
    "fig = P.figure(figsize=(20, 20))\n",
    "for idx in range(len(rand_indices)):\n",
    "    a=fig.add_subplot(1,display_size,idx+1)\n",
    "    P.imshow(train_dataset[rand_indices[idx],:,:, 0], cmap='Greys_r')  ## note that the image id has moved to leftmost\n",
    "    a.set_title(str(train_labels_replace[rand_indices[idx]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## As we can see, these numbers are sometimes in white and other times in black.\n",
    "## we don't want this to interfere with the decision as it is quite irrelevant\n",
    "\n",
    "## lets try some edge detectors\n",
    "\n",
    "from skimage import feature\n",
    "\n",
    "fig = P.figure(figsize=(20, 20))\n",
    "for idx in range(len(rand_indices)):\n",
    "    a=fig.add_subplot(1,display_size,idx+1)\n",
    "    P.imshow(feature.canny(train_dataset[rand_indices[idx],:,:, 0]), cmap='Greys_r')  ## note that the image id has moved to leftmost\n",
    "    a.set_title(str(train_labels_replace[rand_indices[idx]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ok, so we have got the edges now. the 1s appear to be of different thickness. perhaps dilation would help?\n",
    "\n",
    "\n",
    "from skimage import morphology\n",
    "\n",
    "fig = P.figure(figsize=(20, 20))\n",
    "for idx in range(len(rand_indices)):\n",
    "    a=fig.add_subplot(1,display_size,idx+1)\n",
    "    final_image = morphology.skeletonize(feature.canny(train_dataset[rand_indices[idx],:,:, 0]))\n",
    "    P.imshow(final_image, cmap='Greys_r')  ## note that the image id has moved to leftmost\n",
    "    a.set_title(str(train_labels_replace[rand_indices[idx]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## nice! now we can split the train images, train_data into a train and validation test set\n",
    "## we will use the test data as is\n",
    "## select some of the train dataset  as validation set\n",
    "\n",
    "train_images, valid_images, train_labels, valid_labels = \\\n",
    "    ttsplit(train_dataset, train_labels_replace, test_size=0.1, random_state=7)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print train_images.shape\n",
    "print train_images.dtype\n",
    "print train_labels.shape\n",
    "print train_labels.dtype\n",
    "\n",
    "print valid_images.shape\n",
    "print valid_images.dtype\n",
    "print valid_labels.shape\n",
    "print valid_labels.dtype\n",
    "\n",
    "print test_images.shape\n",
    "print test_images.dtype\n",
    "print test_labels.shape\n",
    "print test_labels.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(len(train_label_set)):\n",
    "    print(len(valid_labels[valid_labels == i]))\n",
    "train_additional_images, valid_additional_images, train_additional_labels, valid_additional_labels = \\\n",
    "    ttsplit(extra_images, extra_labels_replace, test_size=0.005, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(len(train_label_set)):\n",
    "    print(len(valid_additional_labels[valid_additional_labels == i]))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_images = np.concatenate((train_images, train_additional_images), axis=0)\n",
    "train_labels = np.concatenate((train_labels, train_additional_labels), axis=0)\n",
    "valid_images = np.concatenate((valid_images, valid_additional_images), axis =0)\n",
    "valid_labels = np.concatenate((valid_labels, valid_additional_labels), axis = 0)\n",
    "\n",
    "print train_images.shape\n",
    "print train_images.dtype\n",
    "print train_labels.shape\n",
    "print train_labels.dtype\n",
    "\n",
    "print valid_images.shape\n",
    "print valid_images.dtype\n",
    "print valid_labels.shape\n",
    "print valid_labels.dtype\n",
    "\n",
    "print test_images.shape\n",
    "print test_images.dtype\n",
    "print test_labels.shape\n",
    "print test_labels.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "remainder = train_images.shape[0]%batch_size\n",
    "if remainder != 0:\n",
    "    train_images = train_images[:-remainder]\n",
    "    train_labels = train_labels[:-remainder]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def randomize(images, labels):\n",
    "    assert(images.shape[0] == labels.shape[0])\n",
    "    perm = np.random.permutation(images.shape[0])\n",
    "    return images[perm], labels[perm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "valid_images, valid_labels = randomize(valid_images, valid_labels)\n",
    "#test_images, test_labels = randomize(test_images, test_labels)\n",
    "train_images, train_labels = randomize(train_images, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print train_images.shape\n",
    "print train_labels.shape[0]%batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"num batches per epoch: {}\".format(train_labels.shape[0]/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## save the data into a pickle file\n",
    "\n",
    "import cPickle as pickle\n",
    "\n",
    "f_to_data = {\n",
    "    'train_single_data.pkl': train_images,\n",
    "    'train_single_labels.pkl': train_labels,\n",
    "    'valid_single_data.pkl': valid_images,\n",
    "    'valid_single_labels.pkl': valid_labels,\n",
    "    'test_single_data.pkl': test_images,\n",
    "    'test_single_labels.pkl': test_labels,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for k,v in f_to_data.items():\n",
    "    f_path = os.path.join(data_path, k)\n",
    "    if not os.path.exists(f_path):\n",
    "        with open(f_path, 'w') as f:\n",
    "            print \"writing {}\".format(f_path)\n",
    "            pickle.dump(v, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## always start a model with what you are going to measure it by\n",
    "def accuracy(predictions, labels):\n",
    "  assert(predictions.shape[0] == len(labels))\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == labels)\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "patch_size = 5\n",
    "depth = 16\n",
    "num_hidden = 64\n",
    "image_size = 32\n",
    "num_channels = 1\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.placeholder(\n",
    "    tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "  tf_train_labels = tf.placeholder(tf.int32, shape=(batch_size))\n",
    "  tf_valid_dataset = tf.constant(valid_images)\n",
    "  tf_test_dataset = tf.constant(test_images)\n",
    "  \n",
    "  # Variables.\n",
    "  layer1_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, num_channels, depth], stddev=0.1)) ## 5 x 5 x 1 x 16 -> 28 x 28 x 1 x 16\n",
    "  layer1_biases = tf.Variable(tf.zeros([depth]))\n",
    "\n",
    "  layer2_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, depth, depth], stddev=0.1))       ## 5 x 5 x 16 x 16 -> 24 x 24 x 16 x 16\n",
    "  layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "\n",
    "  layer3_weights = tf.Variable(tf.truncated_normal(\n",
    "      #[image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1)) \n",
    "    [patch_size * patch_size * depth, num_hidden], stddev=0.1)) \n",
    "  layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "\n",
    "  layer4_weights = tf.Variable(tf.truncated_normal(\n",
    "      [num_hidden, num_labels], stddev=0.1))\n",
    "  layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  \n",
    "  # Model.\n",
    "  def model(data, is_training = False):\n",
    "    conv = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='VALID')\n",
    "    pool = tf.nn.avg_pool(conv, [1, 4, 4, 1], [1,2,2,1], padding = 'SAME')\n",
    "    hidden = tf.nn.relu(pool + layer1_biases)\n",
    "    \n",
    "    conv = tf.nn.conv2d(hidden, layer2_weights, [1, 2, 2, 1], padding='VALID')\n",
    "    pool = tf.nn.max_pool(conv, [1,1,1,1], [1,1,1,1], padding = 'SAME')\n",
    "    hidden = tf.nn.relu(pool + layer2_biases)\n",
    "    shape = hidden.get_shape().as_list()\n",
    "    reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "    if is_training:\n",
    "        hidden  = tf.nn.dropout(hidden, keep_prob=0.5)\n",
    "    return tf.matmul(hidden, layer4_weights) + layer4_biases\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = model(tf_train_dataset, True)\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.sparse_softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "    \n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    \n",
    "  # params for exponential_decay API\n",
    "  # https://www.tensorflow.org/versions/r0.7/api_docs/python/train.html#exponential_decay\n",
    "  # values for learning_rate [0.05, 0.15, 0.2, 0.25, 0.5]\n",
    "\n",
    "  learning_rate = tf.train.exponential_decay(0.15, global_step, 300, 0.99)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "  #optimizer = tf.train.AdagradOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "  test_prediction = tf.nn.softmax(model(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_steps = 75001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    batch_data = train_images[offset:(offset + batch_size), :, :, :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size)]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 2500 == 0):\n",
    "      print('Minibatch loss at step %d: %f' % (step, l))\n",
    "      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  final_predictions = test_prediction.eval()\n",
    "  print('Test accuracy: %.1f%%' % accuracy(final_predictions, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print final_predictions.shape\n",
    "predictions = np.argmax(final_predictions, 1).T\n",
    "print predictions.shape\n",
    "\n",
    "rand_indices = np.random.randint(0, test_images.shape[0], size=display_size)\n",
    "fig = P.figure(figsize=(20, 20))\n",
    "for idx in range(len(rand_indices)):\n",
    "    orig_idx = rand_indices[idx]\n",
    "    a=fig.add_subplot(1,display_size,idx+1)\n",
    "    P.imshow(test_images[orig_idx,:,:,0])\n",
    "    a.set_title(str(predictions[orig_idx]))#rand_indices[idx, 0]]]]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
